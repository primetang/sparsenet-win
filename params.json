{"name":"sparsenet-win","tagline":"Sparse Coding Simulation Software for Windows","body":"###1. Introduction\r\n\r\nThe toolbox is a warpper of [sparsenet](http://redwood.berkeley.edu/bruno/sparsenet/) for Windows, but I think it can also use for Linux without test. The original code developed by Bruno Olshausen, can be found at http://redwood.berkeley.edu/bruno/sparsenet/.\r\n\r\nPlease feel free to contact me [tanggefu@gmail.com] if you have any questions.\r\n\r\n\r\n###2. Compilation\r\n\r\nTo run sparsenet, you will first need to compile the conjugate gradient (cgf) routine so that it may be called through matlab. If you want to test or contribute, [CMAKE](http://www.cmake.org), a cross-platform, open-source build system, is usded to build some tools for the purpose. CMake can be downloaded from [CMake' website](http://www.cmake.org/cmake/resources/software.html).\r\n\r\nDuring compilation, create a new directory named `./nrf/build`, then choose a appropriate compiler and switch to `./nrf/build` directory, finally, execute the following command according to your machine:\r\n\r\n* Windows\r\n\r\n```cpp\r\ncmake -DCMAKE_BUILD_TYPE=Release .. -G\"NMake Makefiles\"\r\nnmake\r\n```\r\n\r\n* Linux\r\n\r\n```cpp\r\ncmake ..\r\nmake\r\n```\r\n\r\nIf successful, this should create a file named cgf.xxx in `./nrf/build/matlab/`, where xxx is a suffix that depends on what machine you are on. Then you should copy it to the main directory.\r\n\r\n\r\n###3. Usage\r\n\r\nAt this point, you are in business. Now startup Matlab.\r\n\r\nFirst load the training data.  You can get the array IMAGES from the sparsenet web page at http://redwood.berkeley.edu/bruno/sparsenet/.\r\n\r\nOnce you download this array, you load into Matlab by typing:\r\n\r\n```matlab\r\n>> load IMAGES\r\n```\r\n\r\nThis will bring in a matrix of 10 images, each 512x512 (it is a 512^2 x 10 array). This consumes about 20 M, so hopefully you have enough memory.  To make your own training dataset, see the instructions in the file `make-your-own-images`.\r\n\r\nThe next step is to define a matrix of basis functions, For example, to learn 64 bases on 8x8 patches, define A as follows:\r\n\r\n```matlab\r\n>> A = rand(64)-0.5;\r\n>> A = A*diag(1./sqrt(sum(A.*A)));\r\n```\r\n\r\nThis will create a 64x64 matrix initialized to random values. Each column is a different basis function that has unit length.\r\n\r\nSet the colormap of Figure 1 to greyscale:\r\n\r\n```matlab\r\n>> figure(1), colormap(gray)\r\n```\r\n\r\nNow simply run the simulation by typing:\r\n\r\n```matlab\r\n>> sparsenet\r\n```\r\n\r\nOnce the simulation starts running, it will display the bases in Figure No. 1 every 10 batches, and it will show the variance of the coefficients in Figure No. 2. You can stop the simulation at any point by typing control-c. You an change the parameters in `sparsenet.m` and then resume execution it again.\r\n\r\nThe learning rate (eta) is initialized to 5.0, which is a large value. This quickly gets the solution in the right ballpark, but it is too large to come to a clean stable solution. Once it looks like you have something interesting emerging, start reducing eta, eventually to about 1.0. A full set of 8x8 bases takes about 15 min. to learn (depending on how fast your workstation is).\r\n\r\n\r\n###4. Notation\r\n\r\n```\r\n* A         Basis functions (Phi in Nature/Vision Research. paper)\r\n* X         Input image (I in Nature/Vision Research paper)\r\n* S         Coefficients (a in Nature/Vision Research paper)\r\n* noise_var noise variance (sigma_N^2 in Vision Research paper, eq. 6)\r\n* beta      steepness of prior (beta in Vision Research paper, eq. 8)\r\n* sigma     scaling parameter for prior (sigma in Nature paper)\r\n* eta       learning rate (eta in Nature/Vision Research paper)\r\n* tol       tolerance for conjugate gradient routine\r\n```\r\n\r\n```\r\n* VAR_GOAL  variance goal for the coefficients\r\n* S_var     actual variance of the coefficients\r\n* var_eta   average rate for S_var\r\n* alpha     gain adaptation rate\r\n* gain      L2 norm of basis functions\r\n```\r\n\r\nNote that in both the Vision Research and Nature papers, sigma_N^2 (noise_var) and beta are combined into a single constant, lambda (eq. 14 VR, eq. 2 Nature). The scale parameter for the coefficients does not appear in the Vision Research paper, but it does appear in the Nature paper (eqs. 4,5).\r\n\r\n\r\n###5. References\r\n\r\n```\r\nOlshausen BA, Field DJ (1997). Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37, 3311-3325.\r\n\r\nOlshausen BA, Field DJ (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381, 607-609.\r\n```\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}